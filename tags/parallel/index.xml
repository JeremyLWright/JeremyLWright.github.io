<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Parallel on Code Strokes</title>
    <link>http://localhost:1313/tags/parallel/</link>
    <description>Recent content in Parallel on Code Strokes</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 23 Oct 2011 17:55:47 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/parallel/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Parallel Game-of-Life</title>
      <link>http://localhost:1313/2011/10/parallel-game-of-life/</link>
      <pubDate>Sun, 23 Oct 2011 17:55:47 +0000</pubDate>
      
      <guid>http://localhost:1313/2011/10/parallel-game-of-life/</guid>
      <description>

&lt;p&gt;Conway&amp;rsquo;s &lt;a href=&#34;http://www.bitstorm.org/gameoflife/&#34;&gt;Game of Life&lt;/a&gt; is a dramatic illustration of emmergent behavior; that a seemingly complex system, such as cell mitosis can be governed by a set of simple rules. OpenMP is a fantastic set of language extensions which allows one to add dramatic parallelism without complex thread management.  As a demonstration of &lt;a href=&#34;http://openmp.org/wp/&#34;&gt;OpenMP&lt;/a&gt;&amp;rsquo;s simplicity I implemented the Game of Life. The code and all analysis is available on &lt;a href=&#34;https://bitbucket.org/jwright/parallel-game-of-life&#34;&gt;bitbucket.org&lt;/a&gt;.&lt;/p&gt;

&lt;!-- more --&gt;

&lt;p&gt;In a real program, it common to have swathes of code which cannot be made parallel. OpenMP&amp;rsquo;s limited perspective on data limits one further.  As such, OpenMP is most effective when the data is decomposed into independently manageable chunks. To evaluate the performance affect of OpenMP on my implementation I used a single Base Class to implement the serial portion of the code. Each child class makes small modifications to the generation calculation, decomposing the data in different ways.  I made three decompositions:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Decomposition by Rows&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Decomposition by Columns&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Random Decomposition by Cell&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Interestingly, the relative performance of each mechanism was quite volatile. The serial version of the code is quite simple. Basically, calculate each live/die action for every cell then, after all calculations are made, commit the updates in one atomic action.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;void GameGrid::CalculateGeneration()
{
    list delayedUpdates;

    for(size_t col = 0; col &amp;lt; GetGridSize(); ++col)
    {
        for(size_t row = 0; row &amp;lt; GetGridSize(); ++row)
        {
            uint32_t livingNeighbors = CountLivingNeighbors(col, row);
#ifdef DEBUG
            cout &amp;lt;&amp;lt; &amp;quot;Row: &amp;quot; &amp;lt;&amp;lt; row
                &amp;lt;&amp;lt; &amp;quot;Col: &amp;quot; &amp;lt;&amp;lt; col
                &amp;lt;&amp;lt; &amp;quot;: &amp;quot; &amp;lt;&amp;lt; livingNeighbors &amp;lt;&amp;lt; endl;
#endif
            Update u;
            u.threadId = omp_get_thread_num();
            u.position = &amp;amp;(Grid[col][row]);
            u.threadPosition = &amp;amp;(GridThreads[col][row]);
            if(Grid[col][row]) //If Cell is alive
            {
                if(livingNeighbors = 4)
                {
                    //Kill Cell
                    u.updateValue = false;
                    delayedUpdates.push_back(u);
                }
                /* else remain alive */
            }
            else
            {
                if(livingNeighbors == 3)
                {
                    //ConceiveCell
                    u.updateValue = true;
                    delayedUpdates.push_back(u);
                }
            }

        }
    }
    commitUpdates(delayedUpdates);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Even this simple routine can generate incredible emergent behavior.&lt;/p&gt;

&lt;p&gt;Your browser does not support the video tag.&lt;/p&gt;

&lt;p&gt;Now, we have a serial platform to extend into a parallel one. First we extend the base class with simple inheritance.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class GameGridParallelCol : public GameGrid
{
    public:
        typedef std::tr1::shared_ptr Ptr; ///@note The Anderson Smart Pointer Idiom
        typedef std::tr1::weak_ptr WeakPtr;
        static GameGridParallelCol::Ptr construct(string filename, size_t size);
        virtual ~GameGridParallelCol();
        virtual void CalculateGeneration();
    protected:
    private:

        GameGridParallelCol(string filename, size_t size);
        GameGridParallelCol::WeakPtr self;

};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The using OpenMP Directives add col decomposition:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;void GameGridParallelCol::CalculateGeneration()
{
    vector delayedUpdates[omp_get_max_threads()]; //Create duplicate update lists, to avoid critical sections.
   for(size_t row = 0; row &amp;lt; GetGridSize(); ++row)
    {
#pragma omp parallel for
        for(size_t col = 0; col &amp;lt; GetGridSize(); ++col)
        {
            uint32_t livingNeighbors = CountLivingNeighbors(col, row);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here is the power of OpenMP, its simplicity.  OpenMP exposes a set of #pragma functions which apply parallelism to the beginning of a scope block, and a barrier at the end of the scope block. Automatic parallelism, it doesn&amp;rsquo;t get easier than this. However OpenMP&amp;rsquo;s simplicity does hamper it in a few ways. OpenMP directives for instance, cannot batch C++ iterators. Now we have a thread pool which will run segments of the game of life in parallel.&lt;/p&gt;

&lt;p&gt;[caption id=&amp;ldquo;attachment_420&amp;rdquo; align=&amp;ldquo;aligncenter&amp;rdquo; width=&amp;ldquo;792&amp;rdquo; caption=&amp;ldquo;Column Order Decomposition&amp;rdquo;]&lt;a href=&#34;http://www.codestrokes.com/wp-content/uploads/2011/10/Screenshot-at-2011-10-22-215034.png&#34;&gt;&lt;img src=&#34;http://www.codestrokes.com/wp-content/uploads/2011/10/Screenshot-at-2011-10-22-215034.png&#34; alt=&#34;&#34; /&gt;
&lt;/a&gt;[/caption]&lt;/p&gt;

&lt;p&gt;The right half of the image illustrates each thread, one for each color, as it updates a section of the game grid.  (Decomposition into rows is left as an exercise). Extending this we can make a fully parallel version.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#pragma omp parallel for collapse(2) schedule(dynamic)
   for(size_t row = 0; row &amp;lt; GetGridSize(); ++row)
    {
        for(size_t col = 0; col &amp;lt; GetGridSize(); ++col)
        {
            uint32_t livingNeighbors = CountLivingNeighbors(col, row);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Rendering the following:&lt;/p&gt;

&lt;p&gt;[caption id=&amp;ldquo;attachment_422&amp;rdquo; align=&amp;ldquo;aligncenter&amp;rdquo; width=&amp;ldquo;792&amp;rdquo; caption=&amp;ldquo;Full data decomposition with dynamic thread balancing&amp;rdquo;]&lt;a href=&#34;http://www.codestrokes.com/wp-content/uploads/2011/10/CorrectedFullThreadRender.png&#34;&gt;&lt;img src=&#34;http://www.codestrokes.com/wp-content/uploads/2011/10/CorrectedFullThreadRender.png&#34; alt=&#34;&#34; /&gt;
&lt;/a&gt;[/caption]&lt;/p&gt;

&lt;p&gt;Now we have a fully parallel version where threads calculate cells at random, as the thread is available. Strangely, this isn&amp;rsquo;t always the fastest algorithm.&lt;/p&gt;

&lt;h2 id=&#34;analysis:d59e23fa831b00ad3ec19e2fe0a18821&#34;&gt;Analysis&lt;/h2&gt;

&lt;p&gt;No single decomposition wins out. As such there is no generalization such that one threading mechanism is faster than another in all cases. The Full threading model was mst consistant, but the Row and Column decompositions actually got slower as more cores were added.&lt;/p&gt;

&lt;p&gt;[caption id=&amp;ldquo;attachment_423&amp;rdquo; align=&amp;ldquo;aligncenter&amp;rdquo; width=&amp;ldquo;695&amp;rdquo; caption=&amp;ldquo;6-core Speedup&amp;rdquo;]&lt;a href=&#34;http://www.codestrokes.com/wp-content/uploads/2011/10/fitted-speedup.png&#34;&gt;&lt;img src=&#34;http://www.codestrokes.com/wp-content/uploads/2011/10/fitted-speedup-1024x768.png&#34; alt=&#34;&#34; /&gt;
&lt;/a&gt;[/caption]&lt;/p&gt;

&lt;p&gt;[caption id=&amp;ldquo;attachment_424&amp;rdquo; align=&amp;ldquo;aligncenter&amp;rdquo; width=&amp;ldquo;695&amp;rdquo; caption=&amp;ldquo;12-core Speedup&amp;rdquo;]&lt;a href=&#34;http://www.codestrokes.com/wp-content/uploads/2011/10/fitted-speedup1.png&#34;&gt;&lt;img src=&#34;http://www.codestrokes.com/wp-content/uploads/2011/10/fitted-speedup1-1024x768.png&#34; alt=&#34;&#34; /&gt;
&lt;/a&gt;[/caption]&lt;/p&gt;

&lt;p&gt;Most interestingly, the row and column performance goes down with more cores, while the full decomposition stays pretty constant. I do not have a reason why this is happening.&lt;/p&gt;

&lt;h2 id=&#34;conclusion:d59e23fa831b00ad3ec19e2fe0a18821&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;OpenMP makes adding parallelism to a serial program easy. The constructs allow one to sprinkle parallel code throughout the program, and the measure the performance boost. Sometimes it may be prudent to dive in deeper and manually thread some section of code, especially when using complex C++ constructs such as iterators, but if OpenMP meets the need, then its a low-cost, cross-platform mechanism.  Furthermore, OpenMP is exposed as a set of #pragma functions. Since #pragmas are by definition an extension to the language, if the compiler doesn&amp;rsquo;t understand the directives, they are simply ignored. This allows one to liberally use OpenMP, and if the compiler of choice doesn&amp;rsquo;t support the OpenMP directives, OpenMP will not break the build. OpenMP is a fantastic tool.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Parallelism in D</title>
      <link>http://localhost:1313/2011/05/parallelism-in-d-bucket-sort-part-2/</link>
      <pubDate>Sun, 29 May 2011 18:02:24 +0000</pubDate>
      
      <guid>http://localhost:1313/2011/05/parallelism-in-d-bucket-sort-part-2/</guid>
      <description>&lt;p&gt;Parallelism, it sounds like a religion, and in some sense it is. Like many facets of software engineering, writing good parallel code is more of an art than a science.  I come from a FPGA background where parallelism is part of the language; part of the culture! The tools are designed to find deadlocks, analyze timing and the language itself is fully aware of parallelism.  The hardware world understands parallelism, yet writing parallel software is still difficult.  D is making some pioneering steps in the right direction for &lt;a href=&#34;http://www.digitalmars.com/d/2.0/phobos/std_parallelism.html&#34;&gt;parallelism&lt;/a&gt;.  I use a parallel implementation of bucket sort to show how D makes writing parallel code, correct.&lt;/p&gt;

&lt;!-- more --&gt;

&lt;p&gt;Parallelism is ingrained in the hardware engineer’s mind. The motivating purpose of parallelism is **performance. **There is simply no other justification for the pains of parallelism except the high performance potential it offers. The FPGA engineer’s tool-chain evolves around this fact.  The tools are designed to find deadlocks, analyze timing; however the most valuable feature is that the language itself is fully aware of parallelism. Take this simple Verilog example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;always @ (posedge clock or negedge reset_n)
begin
    if (reset_n == 1&#39;b0) begin
        counter_out &amp;lt;=#1  4&#39;b0000;
    end
    else if (enable == 1&#39;b1) begin
        counter_out &amp;lt;=#1  counter_out + 1;
        counter_in &amp;lt;=#1 counter_in - 1;
        led_out &amp;lt;=#1 led_out ^ 1;
    end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &amp;lt;= operator is called the non-blocking assignment.  In the example above, all three lines in the else condition execute simultaneously.  This is important; just by reading the code you can see that it is parallel.  Our software languages, C, C++, Java, do not make parallel code obvious.  In these languages parallelism tends to feel like a bolt-on, aftermarket feature that never really flows with the rest of the language, or design idioms.  While researching the background on this article, I found a fantastic write up on &lt;a href=&#34;http://www.futurechips.org/tips-for-power-coders/parallel-programming.html&#34;&gt;What Makes Parallel Programs Hard&lt;/a&gt;.  The author contends that parallel programs are hard because of inter-task dependencies.  This is true, but I would further the point that if the language supported parallelism at its core, as Hardware Description Languages do, writing parallel software wouldn’t be so difficult. Furthermore, if a language offered parallel idioms, duplicating robust parallel code would also be easier.&lt;/p&gt;

&lt;p&gt;HDLs make it obvious that the code is parallel, until D I haven’t seen a language do it quite so well.  &lt;a href=&#34;http://www.codestrokes.com/?p=101&#34;&gt;Bucket Sort Part 1&lt;/a&gt; was a quick introduction to Bucket Sort as an algorithm, but the real power of bucket sort is how easily it can be parallelized. Once the list is segmented or “bucketized” each bucket may be sorted simultaneously.  I wrote a D implementation of this, and parallelism really offers incredible performance here.  Take a look.&lt;a href=&#34;http://www.codestrokes.com/wp-content/uploads/2011/05/threading_compared.png&#34;&gt;&lt;img src=&#34;http://www.codestrokes.com/wp-content/uploads/2011/05/threading_compared_thumb.png&#34; alt=&#34;threading_compared&#34; /&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This compares the runtimes of sorting 10 million numbers using various configurations of bucket sort.  Consistently, the multithreaded version is faster.  So how does D makes this easy?&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;uint[] bucket_sort(uint[] unsorted_data, immutable uint num_buckets)
{
    immutable auto interval = (minPos!(&amp;quot;a &amp;gt; b&amp;quot;)(unsorted_data)[0]/num_buckets)+1;
    auto buckets = new uint[][num_buckets];

    foreach(uint datum; unsorted_data)
    {
        scope(failure) { writefln(&amp;quot;%d %d %d&amp;quot;, datum, interval, num_buckets);}
        buckets[datum/(interval)] ~= datum;
    }

    uint[] s;
    version(MultiThreaded)
    {
        foreach(ref bucket; taskPool.parallel(buckets))
        {
            bucket.sort;
        }

    }
    else
    {
        foreach(uint[] bucket; buckets)
        {
            bucket.sort;
        }
    }

    foreach(uint[] bucket; buckets)
    {
        s ~= bucket;
    }
    return s;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This code has the obviousness we are looking for.  taskPool.parallel comes from the &lt;a href=&#34;http://www.digitalmars.com/d/2.0/phobos/std_parallelism.html&#34;&gt;std.parallelism&lt;/a&gt; module starting in &lt;a href=&#34;http://www.digitalmars.com/d/download.html&#34;&gt;D 2.053&lt;/a&gt;.  Simply, by reading the source code, one can see that this code is parallel.  That’s it. The taskPool.parallel routine automatically divvies out units of work between new threads; more importantly, taskPool.parallel automatically joins all threads them at the end of the foreach scope.&lt;/p&gt;

&lt;p&gt;Using this, we can find the optimal configuration of bucket size for both single-threaded and multi-threaded versions of the code.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.codestrokes.com/wp-content/uploads/2011/05/single_threaded.png&#34;&gt;&lt;img src=&#34;http://www.codestrokes.com/wp-content/uploads/2011/05/single_threaded_thumb.png&#34; alt=&#34;single_threaded&#34; /&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.codestrokes.com/wp-content/uploads/2011/05/multi_threaded.png&#34;&gt;&lt;img src=&#34;http://www.codestrokes.com/wp-content/uploads/2011/05/multi_threaded_thumb.png&#34; alt=&#34;multi_threaded&#34; /&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Interestingly, the optimal setting was different between the multithreaded and single threaded versions with multithreaded at 800 buckets and single threaded at 45,800 buckets.  However we can see from the standard deviation plots a sizable variation within a single configuration’s bucket size, while the average runtimes remains fairly flat.  Ergo, bucket size is not the performance bottle neck, it’s the actual sorting, and parallelism drastically illustrates this in the “Threading Compared” plot.&lt;/p&gt;

&lt;p&gt;D provides two primary multithreading techniques, &lt;a href=&#34;http://www.digitalmars.com/d/2.0/phobos/std_parallelism.html&#34;&gt;std.parallelism&lt;/a&gt;, discussed here, and &lt;a href=&#34;http://www.digitalmars.com/d/2.0/phobos/std_concurrency.html&#34;&gt;std.concurrency&lt;/a&gt; with a powerful message passing framework for effective inter-thread communication.  D makes robust, readable, parallel code, easy and correct.  In our case of bucket sort, with only a single line of code.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
