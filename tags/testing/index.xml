<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Testing on Code Strokes</title>
    <link>http://codestrokes.com/tags/testing/</link>
    <description>Recent content in Testing on Code Strokes</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 22 Sep 2011 22:40:42 +0000</lastBuildDate>
    <atom:link href="http://codestrokes.com/tags/testing/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>The $1,000,000 Compile</title>
      <link>http://codestrokes.com/2011/09/the-1000000-compile/</link>
      <pubDate>Thu, 22 Sep 2011 22:40:42 +0000</pubDate>
      
      <guid>http://codestrokes.com/2011/09/the-1000000-compile/</guid>
      <description>&lt;p&gt;Hitting the compile and run button costs $1,000,000. How would such a cost affect your test strategy? As extreme as that sounds, its a harsh reality for ASIC designers. The start-up costs for an ASIC design are extreme, as a result ASIC verification is very important. Especially since you can&amp;rsquo;t just update a buggy ASIC.  In software engineering, we design increasingly complex systems requiring more and more elegant test strategies.  I propose ASIC verification is in a more mature state, and software engineers could learn a lot from the translation of these techniques.&lt;/p&gt;

&lt;p&gt;&amp;lt;!&amp;ndash; more &amp;ndash;&amp;gt;SystemVerilog, the premier language in the ASIC verification space has an incredible feature I feel has a place in software verification: Random Constraints. Constrains allow the compiler to generate a set of test vectors that match some simple set of rules.  The verification bench may then use these test vectors to test the unit under test.  Conceptually, the random bench will test the device in ways the developer never thought of, either with a new test case, or a series of test cases totally unimaginable by the engineer. Find the bug before you&lt;a href=&#34;http://en.wikipedia.org/wiki/Tape-out&#34;&gt; tape-out &lt;/a&gt;=&amp;gt; keep your job.&lt;/p&gt;

&lt;p&gt;Hardware verification also teaches us that no language feature is a panacea.  Random Constraints are a tool to help us find new test cases, but we need model the behavior of our system to get expected behavior.  Hardware Verification uses a pattern similar to the block diagram presented here. &lt;a href=&#34;http://www.codestrokes.com/wp-content/uploads/2011/09/ASIC-Test-Bench.png&#34;&gt;&lt;img src=&#34;http://www.codestrokes.com/wp-content/uploads/2011/09/ASIC-Test-Bench.png&#34; alt=&#34;&#34; /&gt;
&lt;/a&gt; Notice however that one requires a System Model in addition to the UUT (Unit Under Test. This System Model is ideally a &amp;ldquo;clean-room&amp;rdquo; designed block, entirely separate from the UUT developer&amp;rsquo;s group.  This is difficult to do in smaller teams, but its important to understand that one is essentially designing the module twice. This is different from Mock objects however.  Mock Objects are not pure system models. Mock Objects are standins that implement the interface, but with a listing of recalculated responses.  However, one could use a higher level language for the Model such as python.  Such a language would make the design easier, and make provide insight on how to constraint the test vectors.&lt;/p&gt;

&lt;p&gt;So how does this apply to software? The generic pattern maps perfectly.  Using metaprogramming techniques we can build a driver that generates random constraints. Drive the blocks and iterate through the tests. Publish a report. Compile &amp;amp; Run.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>